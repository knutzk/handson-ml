{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 11: Starting with tensorflow\n",
    "\n",
    "_All credit for the code examples of this notebook goes to the book \"Hands-On Machine Learning with Scikit-Learn & TensorFlow\" by A. Geron. Modifications were made and text was added by K. Zoch in preparation for the hands-on sessions._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "First, import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# Function to save a figure. This also decides that all output files \n",
    "# should stored in the subdirectorz 'classification'.\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "EXERCISE = \"tensorflow\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"output\", EXERCISE, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating graphs\n",
    "\n",
    "As we learnt in the lecture, tensorflow is based on _graph building_. It is important to understand that building a graph such as the one below does _not_ actually execute any computation of the results yet. This is done in a tensorflow _session_. What result would you expect from the created graph below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "x = tf.Variable(5, name=\"x\")\n",
    "y = tf.Variable(4, name=\"y\")\n",
    "f = x*x*y + 3*y - 2*x + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below line displays information about the type of `f`. As you can see, `f` is actually an output _tensor_ –check out the [tf.Tensor](https://www.tensorflow.org/api_docs/python/tf/Tensor) class documentation and also the tensorflow [guide on tensors](https://www.tensorflow.org/guide/tensors). Quite generally, tensorflow's documentation and guide are worth reading, and spending some time on them will help you to understand the basics of the low-level tensorflow API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to execute the above graph, we still need to create a [tf.Session](https://www.tensorflow.org/api_docs/python/tf/Session). Again, there is a very good [guide on graphs and sessions](https://www.tensorflow.org/guide/graphs). We can either initialise a session, call its _run_ function for all tensorflow variables, obtain the result, and close the session afterwards. But using python's `with` keyword takes care of opening and closing the session in a much nicer way (and it makes `sess` within the `with` block the default session, which simplifies our run calls quite a lot):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result = f.eval()\n",
    "print(\"Result of the operation: %s\" % result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid having to intialise _all_ variables by hand, we can create a global initialiser, and then simply need to call the `run` function on it once and evaluate our output tensor, as the following piece of code demonstrates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "result = -1\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    result = f.eval()\n",
    "print(\"Result of the operation: %s\" % result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another alternative, especially when using the interactive python interpreter, is to use an _interactive_ session ([tf.InteractiveSession](https://www.tensorflow.org/api_docs/python/tf/InteractiveSession)).\n",
    "\n",
    "Another important thing to note is that individual graph runs do _not_ share values of individual nodes. They might evaluate to the same values for every single run, but each graph evaluation is done independently. On the one hand, this is great because it enables distribution of operations onto many different machines/resources. On the other hand, in some cases it might be desirable to evaluate two output tensors simultaneously, and to avoid double evaluation of some nodes. This is demonstrated in the following piece of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = tf.constant(5)\n",
    "m = k - 2\n",
    "x = m + 5\n",
    "y = m * 3\n",
    "\n",
    "# Both x and y are evaluated in individual graphs runs.\n",
    "with tf.Session() as sess:\n",
    "    print(\"Resulting values: %s, %s\" % (x.eval(), y.eval()))\n",
    "    \n",
    "# x and y are evaluated simultaneously, i.e. the value\n",
    "# of m is only computed once.\n",
    "with tf.Session() as sess:\n",
    "    x_val, y_val = sess.run([x, y])\n",
    "    print(\"Resulting values: %s, %s\" % (x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "As we did with Scikit-Learn, tensorflow can also perform linear regression. Before trying the gradient descent method, we can build a tensorflow graph using the normal equation. If you don't remember the normal equation and how it can be used to solve linear regression problems, now is the perfect moment to go back and revise the previous content!\n",
    "\n",
    "First, fetch an example dataset provided by Scikit-Learn (this might take a moment ...):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build a tensorflow graph which calculates our theta, that is, our estimators for the values of the parameters minimising the cost function. Check out [tf.linalg.matmul](https://www.tensorflow.org/api_docs/python/tf/linalg/matmul), [tf.transpose](https://www.tensorflow.org/api_docs/python/tf/transpose) and [tf.linalg.inv](https://www.tensorflow.org/api_docs/python/tf/linalg/inv) documentations if you'd like to learn more about the operations performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(theta_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, we can also use the [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) class of Scikit-Learn for the same minimisation task. As you might remember, the syntax for Scikit-Learn would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing.data, housing.target.reshape(-1, 1))\n",
    "\n",
    "print(np.r_[lin_reg.intercept_.reshape(-1, 1), lin_reg.coef_.T])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the parameter values comparable between TF and Scikit-Learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving from the normal equation to batch gradient descent, remember that we need to scale the input features (i.e. normalise them). Of course there are ways to do this in tensorflow, but Scikit-Learn's standard scaler works just as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data = scaler.fit_transform(housing.data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to implement gradient descent. This is fairly straight-forward: Start with randomly distributed theta values for now, calculate the prediction based on the current theta, calculate the error of that prediction, and eventually calculate the gradients based on this error. Then, the operation for each epoch is to update the values of theta with `theta - eta * gradients`, as implemented below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the graph so that we don't use any previous results.\n",
    "reset_graph()\n",
    "\n",
    "# Let's start with a low learning rate and many epochs.\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "# The tf input tensors: X for the input features of the data,\n",
    "# y for the target values (this is a regression problem!).\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "\n",
    "# Start with a randomly initialised set of estimators for theta.\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "\n",
    "# Calculate the predicted values, their errors w.r.t. to the\n",
    "# true values, and the mean squared error (MSE). Remember that\n",
    "# all of this only builds a graph in tf, these are *not*\n",
    "# actual computation operations at the moment.\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "\n",
    "# Calculate the gradients by hand, m is the size of the dataset.\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "\n",
    "# Calculate the gradients using tf's automated differentation.\n",
    "# gradients = tf.gradients(mse, [theta])[0]\n",
    "\n",
    "# And finally define the training operation: update the values\n",
    "# for theta, following the calculated gradients.\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "# Initialise all variables.\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# And open a session to execute the graph.\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "   \n",
    "    # Get the best value for theta by calling eval().\n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, the MSE is decreasing, but what are the best values for theta?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code currently implements the gradients by hand. However, this might not be extremely efficienct in terms of computing for more complex problems. Imagine having to calculate derivatives in many nodes of a neural network ... To avoid duplicate evaluations of derivatives, for example if the chain rule can be applied in some of them, we can make use of tf's sophisticated algorithms for differentation, called _reverse-mode autodiff_. If you haven't heard of automatic differentation and its reverse mode, this would be a good time to read up on it.\n",
    "\n",
    "You can try the magic by replacing the line `gradients = [...]` with the one below, which will activate tf's autodiff algorithm instead. In the above example, this probably doesn't make a huge difference, but in multi-layer neural nets it certainly will."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation with TensorBoard\n",
    "\n",
    "The following section gives you an idea how powerful tf's _TensorBoard_ is. TensorBoard is a tool shipped with tf as a visualisation tool and can be very helpful to debug and/or understand your model better. Again, the tf [guide on TensorBoard](https://www.tensorflow.org/guide/summaries_and_tensorboard) is worth reading.\n",
    "\n",
    "To open TensorBoard, first open a terminal, navigate to the folder, in which this jupyter notebook is placed, and type `tensorboard --logdir tf_logs`. This should give you some terminal output – the TensorBoard instance is started. Then, open a new tab in your browser and type `localhost:6006` in the address bar. You should reach the TensorBoard main page.\n",
    "\n",
    "The following code first sets up the logging of graph runs in tensorflow (this is needed by TensorBoard to visualise them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block sets up a mini-batch gradient descent model trained on the \"housing\" data that we already used in the code examples above. Instead of directly giving the input to input tensors, we need to set up [tf.placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) objects that we can provide at runtime. This is necessary as we only provide the dataset in mini-batches for each training step. The rest essentially works as above (with a few commands added to provide logging information for TensorBoard)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "# Define a small function to create mini-batches.\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    np.random.seed(epoch * n_batches + batch_index)\n",
    "    indices = np.random.randint(m, size=batch_size)\n",
    "    X_batch = scaled_housing_data_plus_bias[indices]\n",
    "    y_batch = housing.target.reshape(-1, 1)[indices]\n",
    "    return X_batch, y_batch\n",
    "\n",
    "# Define placeholders for the input features and target values,\n",
    "# because we need to set them at runtime (since we're working\n",
    "# with mini-batches, not the entire dataset).\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "# Calculate values for theta, the predicted values, the errors\n",
    "# on them and the mean squared errors (MSE's).\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "\n",
    "# Use a GradientDescentOptimizer and define the training operation.\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Set up writing into a file. This is needed for logging\n",
    "# details of the operations for the visualisation.\n",
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "# Start the session run.\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            # Fetch the mini-batch with the function defined before.\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            # Save information for every tenth batch.\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()   \n",
    "    \n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are our estimators for theta now? Are they comparable to what was achieved with batch gradient descent above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now have a look at the TensorBoard tab in your browser (maybe you have to refresh). You should suddenly find _a lot_ of information about both your model, the training procedure etc. Feel free to browse around and explore the features.\n",
    "\n",
    "With more complex models, overview graphs in TensorBoard can become very cluttered. Of course, tensorflow provides functionality to deal with that: name scopes for nodes. They allow to group nodes into different contexts that are collapsed into one node in the graph, but can be expanded if needed. Designing the code modular, avoids duplication of identical nodes that are only valid in a limited scope anyways. Nodes can be shared amongst multiple components of the same graph if needed. Feel free to read up more on this by searching for \"name scope\", \"variable scope\", or \"sharing variables\" (and of course \"tensorflow\")."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nav_menu": {
   "height": "603px",
   "width": "616px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
